# =============================================================================
# OpenAI Configuration
# =============================================================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# =============================================================================
# Model Configuration (per request type)
# =============================================================================
# The centralized OpenAI client (lib/openai/client.ts) uses different models
# for different request types. Each has sensible defaults but can be overridden.
#
# IMPORTANT: GPT-5 series models do NOT support reasoning.effort: "none"
# The minimum is "low". The client handles this automatically.
#
# IMPORTANT: Chat kinds (fast/deep) use the SAME underlying model to ensure
# previous_response_id chaining works. Fast vs Deep is controlled via
# reasoning.effort, not by swapping models.

# Unified chat model for all chained conversations (fast + deep)
# If set, overrides both OPENAI_MODEL_FAST and OPENAI_MODEL_DEEP.
# Default: gpt-5-mini
OPENAI_MODEL_CHAT=gpt-5-mini

# Fast chat responses (branch "fast" mode, quick answers)
# NOTE: If OPENAI_MODEL_CHAT is set, this is ignored for chat requests.
# Default: gpt-5-mini | Reasoning: low | Verbosity: low
OPENAI_MODEL_FAST=gpt-5-mini

# Deep chat responses (main chat, thorough answers)
# NOTE: If OPENAI_MODEL_CHAT is set, this is ignored for chat requests.
# Default: gpt-5-mini | Reasoning: medium | Verbosity: low
OPENAI_MODEL_DEEP=gpt-5-mini

# Summarization tasks (branch merging, on-demand summaries)
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_SUMMARIZE=gpt-5-nano

# Intent classification (detecting chat-retrieval requests)
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_INTENT=gpt-5-nano

# Smart Stacks categorization
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_STACKS=gpt-5-nano

# Chat finder/reranking
# Default: gpt-5-mini | Reasoning: low | Verbosity: low
OPENAI_MODEL_FINDER=gpt-5-mini

# Codex tasks (code generation)
# Default: gpt-5.1-codex-mini | Reasoning: medium | Verbosity: medium
# MUST use a GPT-5 codex model for proper code generation
OPENAI_MODEL_CODEX=gpt-5.1-codex-mini

# =============================================================================
# Chat Finder Configuration
# =============================================================================
# Maximum candidates for lexical pre-filtering (default: 30, max: 60)
OPENAI_CHAT_FINDER_MAX_CANDIDATES=

# Top K results to return from reranking (default: 5)
OPENAI_CHAT_FINDER_TOPK=

# =============================================================================
# Vercel KV Configuration (Recommended for production)
# =============================================================================
# Storage for chat history, codex tasks, and workspace snapshots.
#
# LOCAL DEVELOPMENT:
#   Leave blank - uses in-memory store (data lost on restart, but fine for dev)
#
# VERCEL DEPLOYMENT:
#   Recommended for reliable persistence. Without KV, the app falls back to
#   in-memory storage and shows a warning banner. Data may be lost on restarts.
#
#   To configure:
#   1. Go to your Vercel project dashboard
#   2. Storage → Create Database → KV
#   3. Link to your project (env vars auto-populated)
#   4. Redeploy your app
#
# Data is stored with 7-day TTL to prevent unbounded growth.
# Keys are namespaced by user: u:{demo_uid}:chats:*, u:{demo_uid}:codex:*

KV_REST_API_URL=
KV_REST_API_TOKEN=

# Optional: Read-only token for scenarios where you want to limit write access
# KV_REST_API_READ_ONLY_TOKEN=
