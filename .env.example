# OpenAI Configuration
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Model to use for responses
# Examples: gpt-4o, gpt-4o-mini, gpt-5-nano, gpt-5-medium, o1, o1-mini, o3-mini
OPENAI_MODEL=gpt-4o

# Reasoning effort for fast mode (quick responses)
# Only applies to reasoning models (o1, o3, gpt-5 series) - ignored for gpt-4o and older
# Options: none, low, medium, high
OPENAI_REASONING_FAST=low

# Reasoning effort for deep mode (thorough responses)
# Only applies to reasoning models (o1, o3, gpt-5 series) - ignored for gpt-4o and older
# Options: none, low, medium, high
OPENAI_REASONING_DEEP=medium

# Text output verbosity - controls response length naturally without cutting off mid-sentence
# This is the primary way to control response length (not max_output_tokens)
# Options: low (concise), medium (balanced), high (detailed)
OPENAI_TEXT_VERBOSITY=low

# Maximum output tokens - OPTIONAL, only use as a cost safety limit
# Leave unset to let responses complete naturally (recommended)
# Only set this if you need to limit API costs; it may cause truncation
# OPENAI_MAX_OUTPUT_TOKENS=16384

# Vercel KV Configuration (for chat persistence)
# Leave blank for local development (uses in-memory store)
# On Vercel, these are auto-populated when you link a KV store
KV_REST_API_URL=
KV_REST_API_TOKEN=

# Smart Stacks Configuration (Demo 2)
# Model used for categorizing and summarizing chats in Smart Stacks
# Recommend a fast, cheap model for this task
# Leave blank to use default (gpt-4o-mini)
OPENAI_STACKS_MODEL=

# On-demand Summary Model (Demo 2B)
# Model used for generating chat summaries on-demand
# Used when attaching past chats as context
# Leave blank to use default (gpt-4o-mini)
OPENAI_SUMMARY_MODEL=
