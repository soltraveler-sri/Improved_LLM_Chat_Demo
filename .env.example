# =============================================================================
# OpenAI Configuration
# =============================================================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# =============================================================================
# Model Configuration (per request type)
# =============================================================================
# Each request type uses a specific model with appropriate reasoning settings.
# All models below default to GPT-5 series with proper reasoning support.

# Fast chat responses (quick answers, branch "fast" mode)
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_FAST=gpt-5-nano

# Deep chat responses (thorough answers, main chat mode)
# Default: gpt-5-mini | Reasoning: medium | Verbosity: low
OPENAI_MODEL_DEEP=gpt-5-mini

# Summarization tasks (branch merging, on-demand summaries)
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_SUMMARIZE=gpt-5-nano

# Intent classification (detecting chat-retrieval requests)
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_INTENT=gpt-5-nano

# Smart Stacks categorization
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_STACKS=gpt-5-nano

# Chat finder/reranking
# Default: gpt-5-mini | Reasoning: low | Verbosity: low
OPENAI_MODEL_FINDER=gpt-5-mini

# Codex tasks (code generation)
# Default: gpt-5.1-codex-mini | Reasoning: medium | Verbosity: medium
OPENAI_MODEL_CODEX=gpt-5.1-codex-mini

# =============================================================================
# Chat Finder Configuration
# =============================================================================
# Maximum candidates for lexical pre-filtering (default: 30, max: 60)
OPENAI_CHAT_FINDER_MAX_CANDIDATES=

# Top K results to return from reranking (default: 5)
OPENAI_CHAT_FINDER_TOPK=

# =============================================================================
# Vercel KV Configuration (REQUIRED for production)
# =============================================================================
# Storage for chat history, codex tasks, and workspace snapshots.
#
# LOCAL DEVELOPMENT:
#   Leave blank - uses in-memory store (data lost on restart, but fine for dev)
#
# VERCEL DEPLOYMENT:
#   REQUIRED - without these, the app will fail with a clear error message.
#   To configure:
#   1. Go to your Vercel project dashboard
#   2. Storage → Create Database → KV
#   3. Link to your project (env vars auto-populated)
#
# Data is stored with 7-day TTL to prevent unbounded growth.
# Keys are namespaced by user: u:{demo_uid}:chats:*, u:{demo_uid}:codex:*
KV_REST_API_URL=
KV_REST_API_TOKEN=

# =============================================================================
# Legacy Configuration (deprecated, kept for reference)
# =============================================================================
# These are no longer used by the centralized OpenAI client but kept for reference.
# The new model-per-request-type configuration above is the preferred approach.
#
# OPENAI_MODEL=gpt-4o
# OPENAI_REASONING_FAST=low
# OPENAI_REASONING_DEEP=medium
# OPENAI_TEXT_VERBOSITY=low
# OPENAI_STACKS_MODEL=
# OPENAI_SUMMARY_MODEL=
# OPENAI_CODEX_MODEL=
# OPENAI_CHAT_INTENT_MODEL=
# OPENAI_CHAT_FINDER_MODEL=
