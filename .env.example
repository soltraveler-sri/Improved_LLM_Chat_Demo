# OpenAI Configuration
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# Model to use for responses
# Examples: gpt-4o, gpt-4o-mini, gpt-5-nano, gpt-5-medium, o1, o1-mini, o3-mini
OPENAI_MODEL=gpt-4o

# Reasoning effort for fast mode (quick responses)
# Only applies to reasoning models (o1, o3, gpt-5 series) - ignored for gpt-4o and older
# Options: none, low, medium, high
OPENAI_REASONING_FAST=low

# Reasoning effort for deep mode (thorough responses)
# Only applies to reasoning models (o1, o3, gpt-5 series) - ignored for gpt-4o and older
# Options: none, low, medium, high
OPENAI_REASONING_DEEP=medium

# Text output verbosity - controls response length naturally without cutting off mid-sentence
# This is the primary way to control response length (not max_output_tokens)
# Options: low (concise), medium (balanced), high (detailed)
OPENAI_TEXT_VERBOSITY=low

# Maximum output tokens - OPTIONAL, only use as a cost safety limit
# Leave unset to let responses complete naturally (recommended)
# Only set this if you need to limit API costs; it may cause truncation
# OPENAI_MAX_OUTPUT_TOKENS=16384

# =============================================================================
# Vercel KV Configuration (REQUIRED for production)
# =============================================================================
# Storage for chat history, codex tasks, and workspace snapshots.
#
# LOCAL DEVELOPMENT:
#   Leave blank - uses in-memory store (data lost on restart, but fine for dev)
#
# VERCEL DEPLOYMENT:
#   REQUIRED - without these, the app will fail with a clear error message.
#   To configure:
#   1. Go to your Vercel project dashboard
#   2. Storage → Create Database → KV
#   3. Link to your project (env vars auto-populated)
#
# Data is stored with 7-day TTL to prevent unbounded growth.
# Keys are namespaced by user: u:{demo_uid}:chats:*, u:{demo_uid}:codex:*
KV_REST_API_URL=
KV_REST_API_TOKEN=

# Smart Stacks Configuration (Demo 2)
# Model used for categorizing and summarizing chats in Smart Stacks
# Recommend a fast, cheap model for this task
# Leave blank to use default (gpt-4o-mini)
OPENAI_STACKS_MODEL=

# On-demand Summary Model (Demo 2B)
# Model used for generating chat summaries on-demand
# Used when attaching past chats as context
# Leave blank to use default (gpt-4o-mini)
OPENAI_SUMMARY_MODEL=

# Codex Task Model (Demo 3)
# Model used for generating code changes in @codex tasks
# Recommend a capable coding model
# Leave blank to use default (gpt-4o-mini)
OPENAI_CODEX_MODEL=

# Chat Intent Detection (Demo 2 - Conversational Retrieval)
# Model used for detecting chat-retrieval intent
# Recommend a fast model for quick classification
# Leave blank to use default (gpt-5-nano)
OPENAI_CHAT_INTENT_MODEL=

# Chat Finder (Demo 2 - Conversational Retrieval)
# Model used for reranking chat candidates
# Recommend a fast but capable model
# Leave blank to use default (gpt-5-mini)
OPENAI_CHAT_FINDER_MODEL=

# Maximum candidates for lexical pre-filtering (default: 30, max: 60)
OPENAI_CHAT_FINDER_MAX_CANDIDATES=

# Top K results to return from reranking (default: 5)
OPENAI_CHAT_FINDER_TOPK=
