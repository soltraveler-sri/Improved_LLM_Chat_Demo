# =============================================================================
# OpenAI Configuration
# =============================================================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# =============================================================================
# Model Configuration (per request type)
# =============================================================================
# The centralized OpenAI client (lib/openai/client.ts) uses different models
# for different request types. Each has sensible defaults but can be overridden.
#
# IMPORTANT: GPT-5 series models do NOT support reasoning.effort: "none"
# The minimum is "low". The client handles this automatically.
#
# IMPORTANT: Chat kinds (fast/deep) use the SAME underlying model to ensure
# previous_response_id chaining works. Fast vs Deep is controlled via
# reasoning.effort, not by swapping models.

# Unified chat model for all chained conversations (fast + deep)
# If set, overrides both OPENAI_MODEL_FAST and OPENAI_MODEL_DEEP.
# Default: gpt-5-mini
OPENAI_MODEL_CHAT=gpt-5-mini

# Fast chat responses (branch "fast" mode, quick answers)
# NOTE: If OPENAI_MODEL_CHAT is set, this is ignored for chat requests.
# Default: gpt-5-mini | Reasoning: low | Verbosity: low
OPENAI_MODEL_FAST=gpt-5-mini

# Deep chat responses (main chat, thorough answers)
# NOTE: If OPENAI_MODEL_CHAT is set, this is ignored for chat requests.
# Default: gpt-5-mini | Reasoning: medium | Verbosity: low
OPENAI_MODEL_DEEP=gpt-5-mini

# Summarization tasks (branch merging, on-demand summaries)
# Default: gpt-5-nano | Reasoning: minimal (fallback: low) | Verbosity: low
# Alias: OPENAI_SUMMARY_MODEL (takes priority if set)
OPENAI_SUMMARY_MODEL=gpt-5-nano
# OPENAI_MODEL_SUMMARIZE=gpt-5-nano  # Alternative name (lower priority)

# Intent classification (detecting chat-retrieval requests)
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_INTENT=gpt-5-nano

# Smart Stacks categorization
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_STACKS=gpt-5-nano

# Chat finder/reranking
# Default: gpt-5-mini | Reasoning: low | Verbosity: low
OPENAI_MODEL_FINDER=gpt-5-mini

# Codex tasks (code generation)
# Default: gpt-5.1-codex-mini | Reasoning: medium | Verbosity: medium
# MUST use a GPT-5 codex model for proper code generation
OPENAI_MODEL_CODEX=gpt-5.1-codex-mini

# =============================================================================
# Chat Finder Configuration
# =============================================================================
# Maximum candidates for lexical pre-filtering (default: 30, max: 60)
OPENAI_CHAT_FINDER_MAX_CANDIDATES=

# Top K results to return from reranking (default: 5)
OPENAI_CHAT_FINDER_TOPK=

# =============================================================================
# Redis Configuration (Recommended for production)
# =============================================================================
# Storage for chat history, codex tasks, and workspace snapshots.
#
# LOCAL DEVELOPMENT:
#   Leave blank - uses in-memory store (data lost on restart, but fine for dev)
#
# VERCEL DEPLOYMENT:
#   Recommended for reliable persistence. Without Redis, the app falls back to
#   in-memory storage and shows a warning banner. Data may be lost on restarts.
#
#   The app supports TWO env var patterns (choose one):
#
#   OPTION 1: Upstash Redis (via Vercel Redis integration)
#   -------------------------------------------------------
#   When you add Redis via Vercel's marketplace integration, these env vars
#   are automatically injected:
#     UPSTASH_REDIS_REST_URL=https://xxx.upstash.io
#     UPSTASH_REDIS_REST_TOKEN=xxx
#
#   To configure:
#   1. Go to your Vercel project dashboard
#   2. Storage → Connect Store → Redis (Upstash)
#   3. Link to your project (env vars auto-populated)
#   4. Redeploy your app
#
#   OPTION 2: Vercel KV style
#   -------------------------
#   If you prefer the Vercel KV naming convention:
#     KV_REST_API_URL=https://xxx.upstash.io
#     KV_REST_API_TOKEN=xxx
#
# Data is stored with 7-day TTL to prevent unbounded growth.
# Keys are namespaced by user: u:{demo_uid}:chats:*, u:{demo_uid}:codex:*

# Option 1: Upstash Redis env vars (preferred for Vercel Redis integration)
UPSTASH_REDIS_REST_URL=
UPSTASH_REDIS_REST_TOKEN=

# Option 2: Vercel KV style env vars (also supported)
KV_REST_API_URL=
KV_REST_API_TOKEN=

# Note: You only need to set ONE pair of env vars. The app will detect
# whichever is available (Upstash pattern checked first).
