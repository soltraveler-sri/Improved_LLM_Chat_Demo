# =============================================================================
# OpenAI Configuration
# =============================================================================
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=

# =============================================================================
# Model Configuration (per request type)
# =============================================================================
# The centralized OpenAI client (lib/openai/client.ts) uses different models
# for different request types. Each has sensible defaults but can be overridden.
#
# IMPORTANT: GPT-5 series models do NOT support reasoning.effort: "none"
# The minimum is "low". The client handles this automatically.

# Fast chat responses (branch "fast" mode, quick answers)
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_FAST=gpt-5-nano

# Deep chat responses (main chat, thorough answers)
# Default: gpt-5-mini | Reasoning: medium | Verbosity: low
OPENAI_MODEL_DEEP=gpt-5-mini

# Summarization tasks (branch merging, on-demand summaries)
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_SUMMARIZE=gpt-5-nano

# Intent classification (detecting chat-retrieval requests)
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_INTENT=gpt-5-nano

# Smart Stacks categorization
# Default: gpt-5-nano | Reasoning: low | Verbosity: low
OPENAI_MODEL_STACKS=gpt-5-nano

# Chat finder/reranking
# Default: gpt-5-mini | Reasoning: low | Verbosity: low
OPENAI_MODEL_FINDER=gpt-5-mini

# Codex tasks (code generation)
# Default: gpt-5.1-codex-mini | Reasoning: medium | Verbosity: medium
# MUST use a GPT-5 codex model for proper code generation
OPENAI_MODEL_CODEX=gpt-5.1-codex-mini

# =============================================================================
# Chat Finder Configuration
# =============================================================================
# Maximum candidates for lexical pre-filtering (default: 30, max: 60)
OPENAI_CHAT_FINDER_MAX_CANDIDATES=

# Top K results to return from reranking (default: 5)
OPENAI_CHAT_FINDER_TOPK=

# =============================================================================
# Vercel KV Configuration (REQUIRED for production)
# =============================================================================
# Storage for chat history, codex tasks, and workspace snapshots.
#
# LOCAL DEVELOPMENT:
#   Leave blank - uses in-memory store (data lost on restart, but fine for dev)
#
# VERCEL DEPLOYMENT:
#   REQUIRED - without these, the app will fail with a clear error message.
#   To configure:
#   1. Go to your Vercel project dashboard
#   2. Storage → Create Database → KV
#   3. Link to your project (env vars auto-populated)
#
# Data is stored with 7-day TTL to prevent unbounded growth.
# Keys are namespaced by user: u:{demo_uid}:chats:*, u:{demo_uid}:codex:*
KV_REST_API_URL=
KV_REST_API_TOKEN=
